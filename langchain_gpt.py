# -*- coding: utf-8 -*-
"""langchain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TGPni1_eDAlaHBdCCc0a9FSIugQFdknt

# Document Loading

Retrieval augmented generation
- In retrieval augmented generation (RAG), an LLM retrieves contextual documents from an external dataset as part of its execution.

- This is useful if we want to ask question about specific documents (e.g., our PDFs, a set of videos, etc).
"""

! pip install langchain

pip install openai

pip install python-dotenv

import os
import openai
import sys
sys.path.append('../..')

from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file

! pip install pypdf

"""Sample Data Loading (CS229 lecture pdf)"""

from langchain.document_loaders import PyPDFLoader
loader = PyPDFLoader("https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture01.pdf")
pages = loader.load()

"""Each page is its own unique document
- a document contains text (page_content) and metadata
"""

len(pages)

page = pages[0]

#first 500 characters
print(page.page_content[0:500])

page.metadata

"""Youtube Audio Loader"""

from langchain.document_loaders.generic import GenericLoader
from langchain.document_loaders.parsers import OpenAIWhisperParser
from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader

"""# Question Answering"""

import datetime
current_date = datetime.datetime.now().date()
if current_date < datetime.date(2023, 9, 2):
    llm_name = "gpt-3.5-turbo-0301"
else:
    llm_name = "gpt-3.5-turbo"
print(llm_name)

from langchain.document_loaders import PyPDFLoader

# Load PDF
loaders = [
    # Duplicate documents on purpose - messy data
    PyPDFLoader("https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture01.pdf"),
    PyPDFLoader("https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture02.pdf"),
    PyPDFLoader("https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture03.pdf"),
    PyPDFLoader("https://see.stanford.edu/materials/aimlcs229/transcripts/machinelearning-lecture04.pdf")
]
docs = []
for loader in loaders:
    docs.extend(loader.load())

from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 1500,
    chunk_overlap = 150
)

splits = text_splitter.split_documents(docs)

len(splits)

"""# Embeddings"""

from langchain.embeddings.openai import OpenAIEmbeddings
embedding = OpenAIEmbeddings(openai_api_key='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')

!pip install tiktoken --use-deprecated=legacy-resolver
!pip install chromadb

from langchain.vectorstores import Chroma

persist_directory = 'docs/chroma/'

vectordb = Chroma.from_documents(
    documents=splits,
    embedding=embedding,
    persist_directory=persist_directory
)

print(vectordb._collection.count())

user_question = input("Please enter your question: ")

#question = "What are major topics for this class?"
docs = vectordb.similarity_search(user_question,k=3)
len(docs)

from langchain.chat_models import ChatOpenAI
llm = ChatOpenAI(model_name=llm_name, temperature=0, openai_api_key='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')

"""# Retrival QA Chain"""

from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever()
)

result = qa_chain({"query": user_question})

result["result"]

"""# Memory Incorporation"""

from langchain.memory import ConversationBufferMemory
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

"""## Conversational Retrieval Chain -> incorporates chat history for follow-up questions"""

from langchain.chains import ConversationalRetrievalChain
retriever=vectordb.as_retriever()
qa = ConversationalRetrievalChain.from_llm(
    llm,
    retriever=retriever,
    memory=memory
)

question = "Is probability a class topic?"
result = qa({"question": question})
result['answer']

"""### follow-up question"""

question = "why are those prerequesites needed?"
result = qa({"question": question})
result['answer']